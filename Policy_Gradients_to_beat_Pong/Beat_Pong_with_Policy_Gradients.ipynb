{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a077a4dd5540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL\n",
    "* RL is a Branch of machine learning concerned with taking #sequences of actions\n",
    "* Usually described in terms of agent interacting with a previously unknown environment, trying to maximize cumulative reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL combined with other techniques is powerful\n",
    "Policy Gradients > DQN\n",
    "\n",
    "We'll build a 2 layer fully connected neural network with recieves image pixels, outputs probability of moving UP(stochasticity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Policy gradients have 3 key differences from supervised learning\n",
    "* We don't have the correct labels so as a fake label we substitute the action we happened to sample from the policy \n",
    "* We modulate the loss for each example multiplicatively based on the eventual outcome. Since we want to increase the log probability for actions that worked and decrease it for those that didnt. \n",
    "* runs on a continuously changing dataset (the episodes), scaled by the advantage, and we only want to do one (or very few) \n",
    "#updates based on each sampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = 200\n",
    "batch_size = 10\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward (i.e later rewards are exponentially less important)\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "D = 80*80\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb')) #load from pickled checkpoint\n",
    "else:\n",
    "    model = {}\n",
    "    model['w1'] = np.random.randn(H, D)/np.sqrt(D)\n",
    "    model['w2'] = np.random.randn(H)/np.sqrt(H)\n",
    "#we will update buffers that add up gradients over a batch\n",
    "grad_buffer = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes a single game frame as input. Preprocesses before feeding into model\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel() #flattens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        #if reward at index t is nonzero, reset the sum, since this was a game boundary (pong specific!)\n",
    "        if r[t] != 0: running_add = 0 \n",
    "        #increment the sum \n",
    "        #https://github.com/hunkim/ReinforcementZeroToAll/issues/1\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        #earlier rewards given more value over time \n",
    "        #assign the calculated sum to our discounted reward matrix\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_forward(x):\n",
    "    #matrix multiply input by the first set of weights to get hidden state\n",
    "    #will be able to detect various game scenarios (e.g. the ball is in the top, and our paddle is in the middle)\n",
    "    h = np.dot(model['W1'], x)\n",
    "    #apply an activation function to it\n",
    "    #f(x)=max(0,x) take max value, if less than 0, use 0\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    #repeat process once more\n",
    "    #will decide if in each case we should be going UP or DOWN.\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    #squash it with an activation (this time sigmoid to output probabilities)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h # return probability of taking action 2, and hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. \"\"\"\n",
    "    # eph is array of intermediate hidden states\n",
    "    # epdlopgp modulates the gradient with advantage\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    #Compute derivative hidden. It's the outer product of gradient w/ advatange and weight matrix 2 of 2\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    #apply activation\n",
    "    dh[eph <= 0] = 0 # backpro prelu\n",
    "    #compute derivative with respect to weight 1 using hidden states transpose and input observation\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    #return both derivatives to update weights\n",
    "    return {'W1':dW1, 'W2':dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-161e9641c6a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pong-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#The process gets started by calling reset, which returns an initial observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "#environment\n",
    "env = gym.make(\"Pong-v0\")\n",
    "#Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
    "#The process gets started by calling reset, which returns an initial observation\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "#observation, hidden state, gradient, reward\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "#current reward\n",
    "running_reward = None\n",
    "#sum rewards\n",
    "reward_sum = 0\n",
    "#where are we?\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# begin training\n",
    "while True:\n",
    "    # since we want our policy network to detect modtion\n",
    "    # we use difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "    \n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    aprob, h = policy_forward(x)\n",
    "    #this is the stochastic part \n",
    "    #since not apart of the model, model is easily differentiable\n",
    "    #if it was apart of the model, we'd have to use a reparametrization trick (a la variational autoencoders. so badass)\n",
    "    action = 2 if np.random.uniform() < aprob else 3 \n",
    "    \n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken\n",
    "    \n",
    "    # step the environment and get new measurements\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        episode_number += 1\n",
    "\n",
    "        # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        #each episode is a few dozen games\n",
    "        epx = np.vstack(xs) #obsveration\n",
    "        eph = np.vstack(hs) #hidden\n",
    "        epdlogp = np.vstack(dlogps) #gradient\n",
    "        epr = np.vstack(drs) #reward\n",
    "        xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "        #the strength with which we encourage a sampled action is the weighted sum of all rewards afterwards, but later rewards are exponentially less important\n",
    "        # compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "        #advatnage - quantity which describes how good the action is compared to the average of all the action.\n",
    "        epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        grad = policy_backward(eph, epdlogp)\n",
    "        for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "        # perform rmsprop parameter update every batch_size episodes\n",
    "        #http://68.media.tumblr.com/2d50e380d8e943afdfd66554d70a84a1/tumblr_inline_o4gfjnL2xK1toi3ym_500.png\n",
    "        if episode_number % batch_size == 0:\n",
    "            for k,v in model.iteritems():\n",
    "                g = grad_buffer[k] # gradient\n",
    "                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "                grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "        # boring book-keeping\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print('resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward))\n",
    "        if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "        reward_sum = 0\n",
    "        observation = env.reset() # reset env\n",
    "        prev_x = None\n",
    "    \n",
    "    if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "        print ('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlnd-tf-lab]",
   "language": "python",
   "name": "conda-env-dlnd-tf-lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
