{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Advanced dynamic seq2seq model](https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/2-seq2seq-advanced.ipynb)\n",
    "Now the Encoder is bidirectional and Decoders is implemented using tf.nn.raw_rnn. It feeds previously generated tokens during training as inputs, instead of target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = 20*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "# We implement decoder with tf.nn.raw_rnn and will construct decoder_inputs step by step in the loop.\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(encoder_fw_outputs,encoder_bw_outputs),(encoder_fw_final_state,encoder_bw_final_state) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                                                                                cell_fw = encoder_cell,\n",
    "                                                                                                cell_bw = encoder_cell,\n",
    "                                                                                                inputs = encoder_inputs_embedded,\n",
    "                                                                                                sequence_length=encoder_inputs_length,\n",
    "                                                                                                dtype=tf.float32, time_major=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat(\n",
    "    (encoder_fw_outputs, encoder_bw_outputs), 1)\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to decide how far to run decoder. There are several options for stopping criteria:\n",
    "* Stop after specified number of unrolling steps\n",
    "* Stop after model produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output projection\n",
    "Decoder will contain manually specified by us transition step:\n",
    "> output(t) -> output projection(t) -> prediction(t) (argmax) -> input embedding(t+1) -> input(t+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder via tf.nn.raw_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that standard tf.nn.dynamic_rnn requires all inputs (t, ..., t+n) be passed in advance as a single tensor. \n",
    "\n",
    "What if we want to implement more complex mechanic like when we want decoder to receive previously generated tokens as input at every timestamp (instead of lagged target sequence)?\n",
    "\n",
    "tf.nn.raw_rnn is a way to solve this problem.\n",
    "\n",
    "Main part of specifying RNN with tf.nn.raw_rnn is loop transition function. It defines inputs of step t given outputs and state of step t-1.\n",
    "\n",
    "Loop transition function is a mapping \n",
    "> (time, previous_cell_output, previous_cell_state, previous_loop_state) -> (elements_finished, input, cell_state, output, loop_state). \n",
    "\n",
    "It is called before RNNCell to prepare its inputs and state. Everything is a Tensor except for initial call at time=0 when everything is None (except time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop initial state is function of only encoder_final_state and embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transition function such that previously generated token (as judged in greedy manner by argmax over output projection) is passed as next input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "    def get_next_input():\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()\n",
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function raw_rnn in module tensorflow.python.ops.rnn:\n",
      "\n",
      "raw_rnn(cell, loop_fn, parallel_iterations=None, swap_memory=False, scope=None)\n",
      "    Creates an `RNN` specified by RNNCell `cell` and loop function `loop_fn`.\n",
      "    \n",
      "    **NOTE: This method is still in testing, and the API may change.**\n",
      "    \n",
      "    This function is a more primitive version of `dynamic_rnn` that provides\n",
      "    more direct access to the inputs each iteration.  It also provides more\n",
      "    control over when to start and finish reading the sequence, and\n",
      "    what to emit for the output.\n",
      "    \n",
      "    For example, it can be used to implement the dynamic decoder of a seq2seq\n",
      "    model.\n",
      "    \n",
      "    Instead of working with `Tensor` objects, most operations work with\n",
      "    `TensorArray` objects directly.\n",
      "    \n",
      "    The operation of `raw_rnn`, in pseudo-code, is basically the following:\n",
      "    \n",
      "    ```python\n",
      "    time = tf.constant(0, dtype=tf.int32)\n",
      "    (finished, next_input, initial_state, _, loop_state) = loop_fn(\n",
      "        time=time, cell_output=None, cell_state=None, loop_state=None)\n",
      "    emit_ta = TensorArray(dynamic_size=True, dtype=initial_state.dtype)\n",
      "    state = initial_state\n",
      "    while not all(finished):\n",
      "      (output, cell_state) = cell(next_input, state)\n",
      "      (next_finished, next_input, next_state, emit, loop_state) = loop_fn(\n",
      "          time=time + 1, cell_output=output, cell_state=cell_state,\n",
      "          loop_state=loop_state)\n",
      "      # Emit zeros and copy forward state for minibatch entries that are finished.\n",
      "      state = tf.where(finished, state, next_state)\n",
      "      emit = tf.where(finished, tf.zeros_like(emit), emit)\n",
      "      emit_ta = emit_ta.write(time, emit)\n",
      "      # If any new minibatch entries are marked as finished, mark these.\n",
      "      finished = tf.logical_or(finished, next_finished)\n",
      "      time += 1\n",
      "    return (emit_ta, state, loop_state)\n",
      "    ```\n",
      "    \n",
      "    with the additional properties that output and state may be (possibly nested)\n",
      "    tuples, as determined by `cell.output_size` and `cell.state_size`, and\n",
      "    as a result the final `state` and `emit_ta` may themselves be tuples.\n",
      "    \n",
      "    A simple implementation of `dynamic_rnn` via `raw_rnn` looks like this:\n",
      "    \n",
      "    ```python\n",
      "    inputs = tf.placeholder(shape=(max_time, batch_size, input_depth),\n",
      "                            dtype=tf.float32)\n",
      "    sequence_length = tf.placeholder(shape=(batch_size,), dtype=tf.int32)\n",
      "    inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_time)\n",
      "    inputs_ta = inputs_ta.unstack(inputs)\n",
      "    \n",
      "    cell = tf.contrib.rnn.LSTMCell(num_units)\n",
      "    \n",
      "    def loop_fn(time, cell_output, cell_state, loop_state):\n",
      "      emit_output = cell_output  # == None for time == 0\n",
      "      if cell_output is None:  # time == 0\n",
      "        next_cell_state = cell.zero_state(batch_size, tf.float32)\n",
      "      else:\n",
      "        next_cell_state = cell_state\n",
      "      elements_finished = (time >= sequence_length)\n",
      "      finished = tf.reduce_all(elements_finished)\n",
      "      next_input = tf.cond(\n",
      "          finished,\n",
      "          lambda: tf.zeros([batch_size, input_depth], dtype=tf.float32),\n",
      "          lambda: inputs_ta.read(time))\n",
      "      next_loop_state = None\n",
      "      return (elements_finished, next_input, next_cell_state,\n",
      "              emit_output, next_loop_state)\n",
      "    \n",
      "    outputs_ta, final_state, _ = raw_rnn(cell, loop_fn)\n",
      "    outputs = outputs_ta.stack()\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      cell: An instance of RNNCell.\n",
      "      loop_fn: A callable that takes inputs\n",
      "        `(time, cell_output, cell_state, loop_state)`\n",
      "        and returns the tuple\n",
      "        `(finished, next_input, next_cell_state, emit_output, next_loop_state)`.\n",
      "        Here `time` is an int32 scalar `Tensor`, `cell_output` is a\n",
      "        `Tensor` or (possibly nested) tuple of tensors as determined by\n",
      "        `cell.output_size`, and `cell_state` is a `Tensor`\n",
      "        or (possibly nested) tuple of tensors, as determined by the `loop_fn`\n",
      "        on its first call (and should match `cell.state_size`).\n",
      "        The outputs are: `finished`, a boolean `Tensor` of\n",
      "        shape `[batch_size]`, `next_input`: the next input to feed to `cell`,\n",
      "        `next_cell_state`: the next state to feed to `cell`,\n",
      "        and `emit_output`: the output to store for this iteration.\n",
      "    \n",
      "        Note that `emit_output` should be a `Tensor` or (possibly nested)\n",
      "        tuple of tensors with shapes and structure matching `cell.output_size`\n",
      "        and `cell_output` above.  The parameter `cell_state` and output\n",
      "        `next_cell_state` may be either a single or (possibly nested) tuple\n",
      "        of tensors.  The parameter `loop_state` and\n",
      "        output `next_loop_state` may be either a single or (possibly nested) tuple\n",
      "        of `Tensor` and `TensorArray` objects.  This last parameter\n",
      "        may be ignored by `loop_fn` and the return value may be `None`.  If it\n",
      "        is not `None`, then the `loop_state` will be propagated through the RNN\n",
      "        loop, for use purely by `loop_fn` to keep track of its own state.\n",
      "        The `next_loop_state` parameter returned may be `None`.\n",
      "    \n",
      "        The first call to `loop_fn` will be `time = 0`, `cell_output = None`,\n",
      "        `cell_state = None`, and `loop_state = None`.  For this call:\n",
      "        The `next_cell_state` value should be the value with which to initialize\n",
      "        the cell's state.  It may be a final state from a previous RNN or it\n",
      "        may be the output of `cell.zero_state()`.  It should be a\n",
      "        (possibly nested) tuple structure of tensors.\n",
      "        If `cell.state_size` is an integer, this must be\n",
      "        a `Tensor` of appropriate type and shape `[batch_size, cell.state_size]`.\n",
      "        If `cell.state_size` is a `TensorShape`, this must be a `Tensor` of\n",
      "        appropriate type and shape `[batch_size] + cell.state_size`.\n",
      "        If `cell.state_size` is a (possibly nested) tuple of ints or\n",
      "        `TensorShape`, this will be a tuple having the corresponding shapes.\n",
      "        The `emit_output` value may be  either `None` or a (possibly nested)\n",
      "        tuple structure of tensors, e.g.,\n",
      "        `(tf.zeros(shape_0, dtype=dtype_0), tf.zeros(shape_1, dtype=dtype_1))`.\n",
      "        If this first `emit_output` return value is `None`,\n",
      "        then the `emit_ta` result of `raw_rnn` will have the same structure and\n",
      "        dtypes as `cell.output_size`.  Otherwise `emit_ta` will have the same\n",
      "        structure, shapes (prepended with a `batch_size` dimension), and dtypes\n",
      "        as `emit_output`.  The actual values returned for `emit_output` at this\n",
      "        initializing call are ignored.  Note, this emit structure must be\n",
      "        consistent across all time steps.\n",
      "    \n",
      "      parallel_iterations: (Default: 32).  The number of iterations to run in\n",
      "        parallel.  Those operations which do not have any temporal dependency\n",
      "        and can be run in parallel, will be.  This parameter trades off\n",
      "        time for space.  Values >> 1 use more memory but take less time,\n",
      "        while smaller values use less memory but computations take longer.\n",
      "      swap_memory: Transparently swap the tensors produced in forward inference\n",
      "        but needed for back prop from GPU to CPU.  This allows training RNNs\n",
      "        which would typically not fit on a single GPU, with very minimal (or no)\n",
      "        performance penalty.\n",
      "      scope: VariableScope for the created subgraph; defaults to \"rnn\".\n",
      "    \n",
      "    Returns:\n",
      "      A tuple `(emit_ta, final_state, final_loop_state)` where:\n",
      "    \n",
      "      `emit_ta`: The RNN output `TensorArray`.\n",
      "         If `loop_fn` returns a (possibly nested) set of Tensors for\n",
      "         `emit_output` during initialization, (inputs `time = 0`,\n",
      "         `cell_output = None`, and `loop_state = None`), then `emit_ta` will\n",
      "         have the same structure, dtypes, and shapes as `emit_output` instead.\n",
      "         If `loop_fn` returns `emit_output = None` during this call,\n",
      "         the structure of `cell.output_size` is used:\n",
      "         If `cell.output_size` is a (possibly nested) tuple of integers\n",
      "         or `TensorShape` objects, then `emit_ta` will be a tuple having the\n",
      "         same structure as `cell.output_size`, containing TensorArrays whose\n",
      "         elements' shapes correspond to the shape data in `cell.output_size`.\n",
      "    \n",
      "      `final_state`: The final cell state.  If `cell.state_size` is an int, this\n",
      "        will be shaped `[batch_size, cell.state_size]`.  If it is a\n",
      "        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n",
      "        If it is a (possibly nested) tuple of ints or `TensorShape`, this will\n",
      "        be a tuple having the corresponding shapes.\n",
      "    \n",
      "      `final_loop_state`: The final loop state as returned by `loop_fn`.\n",
      "    \n",
      "    Raises:\n",
      "      TypeError: If `cell` is not an instance of RNNCell, or `loop_fn` is not\n",
      "        a `callable`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.raw_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[8, 8, 3, 3]\n",
      "[6, 8, 3]\n",
      "[4, 9, 6]\n",
      "[6, 8, 9, 7]\n",
      "[9, 7, 8, 4, 9]\n",
      "[7, 9, 6, 4, 9, 3, 7, 8]\n",
      "[9, 9, 2, 3, 9, 6, 6]\n",
      "[9, 4, 8, 7, 6, 7]\n",
      "[3, 6, 2, 8, 2, 3, 5]\n",
      "[5, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                       vocab_lower=2, vocab_upper=10,\n",
    "                                       batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.3040153980255127\n",
      "  sample 1:\n",
      "    input     > [6 7 4 4 4 6 6 2]\n",
      "    predicted > [6 6 6 6 6 6 0 3 3 0 3]\n",
      "  sample 2:\n",
      "    input     > [8 7 6 4 8 0 0 0]\n",
      "    predicted > [3 1 1 6 1 6 1 6 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 4 5 8 8 5 5 0]\n",
      "    predicted > [6 6 6 6 6 0 3 3 0 1 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.4616509974002838\n",
      "  sample 1:\n",
      "    input     > [5 5 2 3 7 0 0 0]\n",
      "    predicted > [5 5 2 3 7 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 5 7 6 6 0 0 0]\n",
      "    predicted > [6 5 7 6 6 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 4 5 7 7 8 0 0]\n",
      "    predicted > [2 4 7 7 7 8 1 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.19868206977844238\n",
      "  sample 1:\n",
      "    input     > [7 3 8 4 6 0 0 0]\n",
      "    predicted > [7 3 8 4 6 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 4 2 3 2 9 0 0]\n",
      "    predicted > [6 4 2 3 2 9 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 3 9 3 5 4 0 0]\n",
      "    predicted > [4 3 9 3 5 4 1 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.08551758527755737\n",
      "  sample 1:\n",
      "    input     > [6 9 2 9 2 3 0 0]\n",
      "    predicted > [6 9 2 9 2 3 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 3 6 6 7 0 0 0]\n",
      "    predicted > [4 3 6 6 7 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 4 5 3 8 9 7 0]\n",
      "    predicted > [5 4 5 3 8 9 7 1 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loss_track = []\n",
    "    max_batches = 3001\n",
    "    batches_in_epoch = 1000\n",
    "\n",
    "    try:\n",
    "        for batch in range(max_batches):\n",
    "            fd = next_feed()\n",
    "            _, l = sess.run([train_op, loss], fd)\n",
    "            loss_track.append(l)\n",
    "\n",
    "            if batch == 0 or batch % batches_in_epoch == 0:\n",
    "                print('batch {}'.format(batch))\n",
    "                print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "                predict_ = sess.run(decoder_prediction, fd)\n",
    "                for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                    print('  sample {}:'.format(i + 1))\n",
    "                    print('    input     > {}'.format(inp))\n",
    "                    print('    predicted > {}'.format(pred))\n",
    "                    if i >= 2:\n",
    "                        break\n",
    "                print()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0914 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12272ccf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlnd-tf-lab]",
   "language": "python",
   "name": "conda-env-dlnd-tf-lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
