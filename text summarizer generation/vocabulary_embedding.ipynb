{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json_lines\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "contents = []\n",
    "with open('sample-1M.jsonl', 'rb') as f:\n",
    "    for item in json_lines.reader(f):\n",
    "        titles.append(item['title'].lower())\n",
    "        contents.append(item['content'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/tokens.pkl', 'wb') as fp:\n",
    "    pickle.dump((titles, contents), fp, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('data/tokens.pkl', 'rb') as fp:\n",
    "#     titles, contents = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumpshot gives marketers renewed visibility into paid and organic keywords with launch of jumpshot elite'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New Product Gives Marketers Access to Real Keywords, Conversions and Results Along With 13 Months of Historical Data \\n\\nSAN FRANCISCO, CA -- (Marketwired) -- 09/17/15 -- Jumpshot, a marketing analytics company that uses distinctive data sources to paint a complete picture of the online customer journey, today announced the launch of Jumpshot Elite, giving marketers insight into what their customers are doing the 99% of the time they\\'re not on your site. For years, marketers have been unable to see what organic and paid search terms users were entering, much less tie those searches to purchases. Jumpshot not only injects that user search visibility back into the market, but also makes it possible to tie those keywords to conversions -- for any web site. \\n\\n\"Ever since search engines encrypted search results, marketers have been in the dark about keywords, impacting not only the insight into their own search investments, but also their ability to unearth high converting keywords for their competitors,\" said Deren Baker, CEO of Jumpshot. \"Our platform eliminates the hacks, assumptions, and guesswork that marketers are doing now and provides real data: actual searches tied to actual conversions conducted by real people with nothing inferred.\" \\n\\nUnlike other keyword research tools that receive data through the Adwords API or send bots to cobble together various data inputs and implied metrics, Jumpshot leverages its panel of over 115 million global consumers to analyze real search activity. As a result, Jumpshot is able to provide companies with actionable data to improve the ROI of their search marketing campaigns, SEO tactics and content marketing initiatives. \\n\\nAvailable today, Jumpshot Elite provides 13 months of backward-looking data as well as: \\n\\nAccess to real queries used by searchers \\n\\nPaid and organic results for any website \\n\\nVisibility into organic keywords, eliminating the \"not provided\" outcome in web analytics \\n\\nReal user queries, clicks and transactions instead of machine-generated clicks with inferred results \\n\\nAbility to tie keywords to real transactions on any website \\n\\nVariable attribution models and lookback windows \\n\\nLaunched in January, 2015, Jumpshot grew out of the ambitions of a group of smart marketers and data scientists who were frustrated about the limitations of the data they had access to, and excited about the opportunity to provide new insights into online behavior. \\n\\nThe company uses distinctive data sources to paint a complete picture of the online world for businesses, from where customers spend time online to what they do there and how they get from place to place. By tracking the online customer journey down to each click, Jumpshot reveals how and why customers arrive at purchase decisions. The company tracks more data in more detail than other services, tracking 160 billion monthly clicks generated by its extensive data panel. \\n\\nAbout Jumpshot \\n\\nJumpshot is a marketing analytics platform that reveals the entire customer journey -- from the key sources of traffic to a site, to browsing and buying behavior on any domain. With a panel of 115 million users, Jumpshot provides marketers with the insight to understand what their customers are doing the 99% of the time they\\'re not on their own site -- a scope of information never before attainable. Jumpshot was founded in 2015 and is headquartered in San Francisco. \\n\\nFor more information, please visit www.jumpshot.com. \\n\\nImage Available: http://www2.marketwire.com/mw/frame_mw?attachid=2889222 \\n\\nKelly Mayes \\n\\nThe Bulleit Group \\n\\n615-200-8845 \\n\\nPublished Sep. 17, 2015 \\n\\nCopyright © 2015 SYS-CON Media, Inc. — All Rights Reserved. \\n\\nSyndicated stories and blog feeds, all rights reserved by the author.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_vocab(lst):\n",
    "    vocabcount = Counter(w for txt in lst for w in txt.split())\n",
    "    vocab = list(map(lambda x: x[0], sorted(vocabcount.items(), key = lambda x: x[1], reverse=True)))\n",
    "    return vocab, vocabcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab, vocabcount = get_vocab(titles+contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty = 0\n",
    "eos = 1\n",
    "start_idx = eos + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2idx = {word: idx+start_idx for idx, word in enumerate(vocab)}\n",
    "word2idx['<empty>'] = empty\n",
    "word2idx['<eos>'] = eos\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4900336"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_name = './data/glove.6B.100d.txt'\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "400,000 GloVe symbols\n"
     ]
    }
   ],
   "source": [
    "glove_n_symbols = sum(1 for line in open(glove_name))\n",
    "print()\n",
    "print('{:,} GloVe symbols'.format(glove_n_symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "globale_scale = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(glove_name, 'r') as fp:\n",
    "    i = 0\n",
    "    for l in fp:\n",
    "        l = l.strip().split()\n",
    "        w = l[0]\n",
    "        glove_index_dict[w] = i\n",
    "        glove_embedding_weights[i, :] = list(map(float, l[1:]))\n",
    "        i += 1\n",
    "glove_embedding_weights *= globale_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe std dev: 0.0408\n"
     ]
    }
   ],
   "source": [
    "print('GloVe std dev: {:.4f}'.format(glove_embedding_weights.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w, i in glove_index_dict.items():\n",
    "    w = w.lower()\n",
    "    if w not in glove_index_dict:\n",
    "        glove_index_dict[w] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random-embedding/glove scale: 0.0707 std: 0.0408\n"
     ]
    }
   ],
   "source": [
    "# generate random embedding with same scale as glove\n",
    "np.random.seed()\n",
    "shape = (vocab_size, embedding_dim)\n",
    "scale = glove_embedding_weights.std() * np.sqrt(12) / 2  # uniform and not normal\n",
    "embedding = np.random.uniform(low=-scale, high=scale, size=shape)\n",
    "print('random-embedding/glove scale: {:.4f} std: {:.4f}'.format(scale, embedding.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens, in small vocab: 25,982 found in glove and copied to embedding: 0.6495\n"
     ]
    }
   ],
   "source": [
    "# copy from glove weights of words that appear in our short vocabulary\n",
    "c = 0\n",
    "for i in range(vocab_size):\n",
    "    w = idx2word[i]\n",
    "    g = glove_index_dict.get(w, glove_index_dict.get(w.lower()))\n",
    "    if g is None and w.startswith('#'):  # glove has no hastags (I think...)\n",
    "        w = w[1:]\n",
    "        g = glove_index_dict.get(w, glove_index_dict.get(w.lower()))\n",
    "    if g is not None:\n",
    "        embedding[i, :] = glove_embedding_weights[g, :]\n",
    "        c += 1\n",
    "print('number of tokens, in small vocab: {:,} found in glove and copied to embedding: {:.4f}'.format(c, c / float(vocab_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# of GloVe substitutes found: 145,438\n"
     ]
    }
   ],
   "source": [
    "glove_thr = 0.5\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "word2glove = {}\n",
    "for w in word2idx:\n",
    "    if w in glove_index_dict:\n",
    "        g = w\n",
    "    elif w.lower() in glove_index_dict:\n",
    "        g = w.lower()\n",
    "    elif w.startswith('#') and w[1:] in glove_index_dict:\n",
    "        g = w[1:]\n",
    "    elif w.startswith('#') and w[1:].lower() in glove_index_dict:\n",
    "        g = w[1:].lower()\n",
    "    else:\n",
    "        continue\n",
    "    word2glove[w] = g\n",
    "\n",
    "\n",
    "# for every word outside the embedding matrix find the closest word inside the mebedding matrix.\n",
    "# Use cos distance of GloVe vectors.\n",
    "# Allow for the last `nb_unknown_words` words inside the embedding matrix to be considered to be outside.\n",
    "# Dont accept distances below `glove_thr`\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "normed_embedding = embedding / np.array(\n",
    "    [np.sqrt(np.dot(gweight, gweight)) for gweight in embedding])[:, None]\n",
    "\n",
    "nb_unknown_words = 100\n",
    "\n",
    "glove_match = []\n",
    "for w, idx in word2idx.items():\n",
    "    if idx >= vocab_size - nb_unknown_words and w.isalpha() and w in word2glove:\n",
    "        gidx = glove_index_dict[word2glove[w]]\n",
    "        gweight = glove_embedding_weights[gidx, :].copy()\n",
    "        # find row in embedding that has the highest cos score with gweight\n",
    "        gweight /= np.sqrt(np.dot(gweight, gweight))\n",
    "        score = np.dot(normed_embedding[:vocab_size - nb_unknown_words], gweight)\n",
    "        while True:\n",
    "            embedding_idx = score.argmax()\n",
    "            s = score[embedding_idx]\n",
    "            if s < glove_thr:\n",
    "                break\n",
    "            if idx2word[embedding_idx] in word2glove:\n",
    "                glove_match.append((w, embedding_idx, s))\n",
    "                break\n",
    "            score[embedding_idx] = -1\n",
    "glove_match.sort(key=lambda x: -x[2])\n",
    "print()\n",
    "print('# of GloVe substitutes found: {:,}'.format(len(glove_match)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 100)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5000 zyrtec => immunotherapy\n",
      "0.5000 colonizing => migrating\n",
      "0.5000 cheapskate => dude\n",
      "0.5000 reoffend => 2014/15\n",
      "0.5000 toadfish => braf\n",
      "0.5000 maia => joanna\n",
      "0.5000 jammy => fruity\n",
      "0.5000 neurogenetics => mhealth\n",
      "0.5000 capsa => astra\n",
      "0.5000 tesa => fitbit\n"
     ]
    }
   ],
   "source": [
    "# manually check that the worst substitutions we are going to do are good enough\n",
    "for orig, sub, score in glove_match[-10:]:\n",
    "    print('{:.4f}'.format(score), orig, '=>', idx2word[sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FN = 'vocabulary-embedding'\n",
    "# build a lookup table of index of outside words to index of inside words\n",
    "glove_idx2idx = dict((word2idx[w], embedding_idx) for w, embedding_idx, _ in glove_match)\n",
    "\n",
    "with open('./data/{}.pkl'.format(FN), 'wb') as fp:\n",
    "    pickle.dump((embedding, idx2word, word2idx, glove_idx2idx), fp, 2)\n",
    "\n",
    "# Data\n",
    "Y = [[word2idx[token] for token in headline.split()] for headline in titles]\n",
    "\n",
    "X = [[word2idx[token] for token in d.split()] for d in contents]\n",
    "\n",
    "with open('./data/{}.data.pkl'.format(FN), 'wb') as fp:\n",
    "    pickle.dump((X, Y), fp, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
